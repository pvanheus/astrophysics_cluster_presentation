%% LaTeX Beamer presentation template (requires beamer package)
%% see http://bitbucket.org/rivanvx/beamer/wiki/Home
%% idea contributed by H. Turgut Uyar
%% template based on a template by Till Tantau
%% this template is still evolving - it might differ in future releases!

\documentclass[handout]{beamer}

\mode<presentation>
{
\usetheme{Warsaw}

\setbeamercovered{transparent}
}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{pgf}
\usepackage{setspace}
\usepackage{listings}
\usepackage{hyperref}
\usepackage[round]{natbib}
\renewcommand{\cite}[1]{{\small\citep{#1}}} 
\def\newblock{\hskip .11em plus .33em minus .07em}
\usepackage{numprint}

% font definitions, try \usepackage{ae} instead of the following
% three lines if you don't like this look
\usepackage{mathptmx}
\usepackage[scaled=.90]{helvet}
\usepackage{courier}


\usepackage[T1]{fontenc}

\pgfdeclareimage[height=0.15\textheight]{UWC-logo}{images/UWClogo.png}
\logo{\href{http://www.uwc.ac.za}{\pgfuseimage{UWC-logo}}}

% \setbeamertemplate{sidebar left}{%
%    \vfill%
%    \rlap{\hskip0.1cm%
%          \href{http://www.uwc.ac.za}%
%          {\pgfuseimage{UWC-logo}}}%
%    \vskip2pt%
%    \llap{\usebeamertemplate***{navigation symbols}\hskip0.1cm}%
%    \vskip2pt%
% } 

\title{UWC Astrophysics Cluster}

\subtitle{Low Cost High Performance Computing}

% - Use the \inst{?} command only if the authors have different
%   affiliation.
%\author{F.~Author\inst{1} \and S.~Another\inst{2}}
\author{\texorpdfstring{Peter~van~Heusden\newline
~pvh@sanbi.ac.za}{Peter~van~Heusden}}


% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.
\institute[UWC]
{
Information and Communication Services\\
University of the Western Cape\\
Bellville, South Africa\\
\insertlogo}

\date{September 2014}

%\beamerdefaultoverlayspecification{<+->}


% This is only inserted into the PDF information catalog. Can be left
% out.
\subject{Talks}



% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
% \AtBeginSubsection[]
% {
% \begin{frame}<beamer>
% \frametitle{Outline}
% \tableofcontents[currentsection,currentsubsection]
% \end{frame}
% }

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command:

%\beamerdefaultoverlayspecification{<+->}
\setbeamertemplate{bibliography item}[text]

\begin{document}

\begin{frame}
\logo{}
\titlepage
\end{frame}

\section{Executive Summary}
\begin{frame}
\frametitle{UWC Astrophysics Cluster}
\begin{itemize}
\item 23 worker nodes, 2 master nodes in two clusters
\item Built from Supermicro servers, AMD Opteron processors
\pause
\item Timon cluster: 
\begin{itemize}
\item 4 worker nodes, 192 cores
\item 1024 GB RAM
\item 39 TB filesystem distributed over worker nodes (CephFS)
\end{itemize}
\pause
\item{Pumbaa cluster:}
\begin{itemize}
\item 19 worker nodes, 912 cores
\item 4864 GB RAM
\item 133 /data distributed filesystem (GlusterFS)
\end{itemize}
\end{itemize}
\end{frame}

\pgfdeclareimage[height=0.8\textheight]{Cluster-Diagram}{images/cluster_diagram.png}
\begin{frame}[fragile]
\frametitle{Astrophysics Cluster Diagram}
\begin{columns}
\column{1.2\textwidth}{
  % \hspace{-1cm}
  \pgfuseimage{Cluster-Diagram}
}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Networking}
\begin{itemize}
\item Each cluster has a 10 Gb Ethernet and 1 Gb Ethernet switch
\item 1 GbE provides management layer
\begin{itemize}
\item Used for initial software installation, authentication and cluster management
\end{itemize}
\item 10 GbE used for parallel computing and distributed filesystem
\begin{itemize}
\item Each cluster node (and master) has dual 10GbE twisted pair connections
\item Allows for independent data paths for MPI (parallel computing) and filesystem (storage)
\end{itemize}
\item Two clusters are connected by VLAN operating on ICS network core switch
\begin{itemize}
\item No high throughput or low latency traffic between clusters
\end{itemize}
\end{itemize}
\end{frame}

\section{Cluster Deployment and Management}

\pgfdeclareimage[width=\textwidth]{Cluster-Management}{images/cluster_management.png}
\begin{frame}
\frametitle{Cluster Deployment and Management}
\pgfuseimage{Cluster-Management}
\end{frame}

\begin{frame}
\frametitle{Cluster Deployment}
\begin{itemize}
\item All cluster nodes run CentOS 6.5
\item Installation was done using \textbf{cobbler}:
\begin{itemize}
\item cobbler on master1 manages install images
\item DHCP and TFTP servers on master1 provide images to install when machines boot using PXE boot
\item Allows for hands-free install of base operating system: only manual intervention is post-installation networking config
\end{itemize}
\item Post-install configuration done using \textbf{ansible}
\begin{itemize}
\item ansible allows config to be scripted and pushed out to whole cluster in single command
\end{itemize}
\item Timon cluster was prototype environment, lessons from Timon deployment were used to streamline larger Pumbaa deployment
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Cluster Management}
\begin{itemize}
\item Manager servers are virtual machines that themselves run on master1 and master2 nodes
\begin{itemize}
\item This allows different software roles to be deployed on independent machines, with no clashing dependencies
\item By using virtual machines, the resource limits (disk/CPU/RAM) of management servers can be specified up front
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Cluster Authentication and Name Services}
\begin{itemize}
\item All machines in cluster operate within \textbf{astroclust.uwc.ac.za} domain and private IP network space
\item Authentication is managed by \textbf{FreeIPA} running on auth1 and auth2 VMs
\item FreeIPA combines LDAP server (389DS), DNS server (bind) and Kerberos to manager user and machine identity and authorisation
\item User and machine records can be created and edited using command-line tools or web interface
\item UWC DNS provides secondary DNS, and cluster queries UWC DNS, ensuring that DNS continues to work even if master1 or master2 are down
\item FreeIPA servers are configured with master-master replication, allowing failover in case one of the authentication servers is down
\end{itemize}
\end{frame}

\section{Cluster Computing}

\begin{frame}
\frametitle{Cluster Scheduler environment}
\begin{itemize}
\item Computing is managed using the \textbf{Torque} batch scheduler
\begin{itemize}
\item Extremely common scheduler in HPC environments: ensures that users can re-use existing skills
\item Integrates with OpenMPI for parallel job execution
\end{itemize}
\item Software installations are managed using the \textbf{Environment Modules} package
\begin{itemize}
\item Scientific software is installed on /opt filesystem that is shared across all compute nodes in a cluster
\item Environment can be customized using \textbf{module add} command 
\end{itemize}

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Running jobs on the cluster}
\begin{itemize}
\item Single manager node (manager1) controls execution on both clusters
\begin{itemize}
\item Clusters operate as independent computing units, with nodes on Timon and Pumbaa being addressed by different batch queues
\item Small user community has meant that scheduling and resource allocation can be kept simple
\end{itemize}
\item \textbf{OpenMPI} allows MPI-enabled programs to run in parallel, using up to 912 CPU cores (whole of Pumbaa)
\begin{itemize}
\item MPI locked to always use 10GbE network
\item In future will partition 10GbE as separate MPI and storage VLANs to ensure that MPI latency is kept as low as possible
\end{itemize}
\end{itemize}
\end{frame}

\section{Cluster Storage}

\begin{frame}
\frametitle{Cluster Storage}
\begin{itemize}
\item Cluster purchased without providing external storage
\item This required investigation of distributed filesystems (DFS):
\begin{itemize}
\item Storage is distributed across the worker nodes in each cluster
\end{itemize}
\item Three different DFS solutions were investigated:
\begin{itemize}
\item \textbf{MooseFS} (didn't scale to 10GbE speeds)
\item \textbf{CephFS} (in use on Timon)
\item \textbf{GlusterFS} (in use on Pumbaa)
\end{itemize}
\item Further investigation: \textbf{BeeGFS} on Timon
\end{itemize}
\end{frame}

\pgfdeclareimage[width=0.9\textwidth]{Timon-Ceph}{images/ceph_cluster.png}
\begin{frame}
\frametitle{CephFS (on Timon)}
\pgfuseimage{Timon-Ceph}
\end{frame}

\begin{frame}
\frametitle{CephFS architecture}
\begin{itemize}
\item \textbf{Ceph} is an open source distributed object store and filesystem
\begin{itemize}
\item Object storage on disk is managed by Object Storage Daemons (OSDs)
\item Objects are looked up by querying a distributed hash table that is managed by Monitor Daemons (MONs)
\item The CephFS filesystem translates filesystem paths to object IDs using Metadata Server Daemons (MDSs)
\end{itemize}
\item{On Timon each server is a client and mounts the distributed filesystem that provides /data, /home and /opt}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Problems with Ceph on Timon}
\begin{itemize}
\item A cluster as small as Timon doesn't need 5 MONs, 3 would be adequate
\item Mounting /home as a DFS means that running jobs and interactive users on master1 share the same filesystem
\begin{itemize}
\item Interactive usage patterns (e.g. metadata access for commands such as ls) conflict with the bulk data writes of HPC jobs
\item As a result users experience poor response when using master1 interactively
\end{itemize}
\item CephFS current release is buggy e.g. files become invisible
\begin{itemize}
\item This might change in the next 12 months: RedHat, purchased Inktank, the original Ceph developers, and have devoted more developers to CephFS
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Future of storage for Timon cluster}
\begin{itemize}
\item CephFS will be replaced, possibly by \textbf{BeeGFS}
\begin{itemize}
\item BeeGFS (formerly FhGFS) is a high-performance parallel filesystem
\end{itemize}
\item BeeGFS promises better metadata performance than GlusterFS and more stability than CephFS
\item Caveat: BeeGFS lacks high availability (HA) features that CephFS and GlusterFS provide, but thus far we've had good uptime and HA is low priority
\end{itemize}
\end{frame}

\end{document}

